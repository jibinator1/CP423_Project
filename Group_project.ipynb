{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1534f99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import soundfile as sf\n",
    "from pyannote.audio import Pipeline\n",
    "from groq import Groq\n",
    "from supabase import create_client\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d107bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CONFIGURATION ---\n",
    "GROQ_API_KEY = \"YOUR_KEY\"\n",
    "SUPABASE_URL = \"YOUR_KEY\"\n",
    "SUPABASE_KEY = \"YOUR_KEY\"\n",
    "HF_AUTH_TOKEN = \"YOUR_KEY\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5b135a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Initializing Clinical IR System (v4.0.4) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 103/103 [00:00<00:00, 875.40it/s, Materializing param=pooler.dense.weight]                             \n",
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- FETCHING FULL SPEAKER-SEPARATED TRANSCRIPT ---\n",
      "[CLINICIAN]: Hello, this is Dr. Smith. We are reviewing the patient's record for today's follow-up on their hypertension.\n",
      "[OTHER]: Thanks, doctor. I've been taking the medication, but I notice some dizziness in the morning, usually right after I wake up.\n",
      "[OTHER]: Dizziness can be a side effect. Let's check your blood pressure now.\n",
      "[CLINICIAN]: It looks like 140 over 90, which is still a bit high.\n",
      "[OTHER]: Thank you.\n",
      "\n",
      "--- GENERATING LLM SUMMARY ---\n",
      "\n",
      "--- CLINICAL SUMMARY ---\n",
      " Here is the summary of the clinical interview:\n",
      "\n",
      "1. Patient Reported Symptoms:\n",
      "   - Dizziness in the morning, usually right after waking up\n",
      "   - The patient reports taking their hypertension medication as prescribed\n",
      "\n",
      "2. Clinician Observations/Questions:\n",
      "   - The patient's blood pressure is 140 over 90, which is still high\n",
      "   - The clinician notes that dizziness can be a side effect of the medication\n",
      "\n",
      "3. Follow-up Plan:\n",
      "   - None explicitly stated in the transcript, but implied that the clinician may need to adjust the patient's medication or provide further guidance to manage the dizziness and high blood pressure.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class ClinicalIRSystem:\n",
    "    def __init__(self):\n",
    "        print(\"--- Initializing Clinical IR System (v4.0.4) ---\")\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # 1. AI Clients\n",
    "        self.groq_client = Groq(api_key=GROQ_API_KEY)\n",
    "        self.supabase = create_client(SUPABASE_URL, SUPABASE_KEY)\n",
    "        \n",
    "        # 2. Embedding Model (all-MiniLM-L6-v2)\n",
    "        self.embed_model = SentenceTransformer('all-MiniLM-L6-v2', device=self.device)\n",
    "        \n",
    "        # 3. Diarization Pipeline [cite: 28, 42]\n",
    "        self.diarization_pipeline = Pipeline.from_pretrained(\n",
    "            \"pyannote/speaker-diarization-community-1\", \n",
    "            token=HF_AUTH_TOKEN\n",
    "        ).to(self.device)\n",
    "\n",
    "    def process_audio_file(self, audio_path, role_mapping):\n",
    "        \"\"\"Processes audio: Diarization -> Transcription -> Indexing [cite: 22]\"\"\"\n",
    "        print(f\"Step 1: Reading {audio_path}...\")\n",
    "        data, samplerate = sf.read(audio_path)\n",
    "        waveform = torch.tensor(data).float()\n",
    "        \n",
    "        if len(waveform.shape) == 1:\n",
    "            waveform = waveform.unsqueeze(0)\n",
    "        elif waveform.shape[0] > waveform.shape[1]:\n",
    "            waveform = waveform.T\n",
    "            \n",
    "        audio_payload = {\"waveform\": waveform, \"sample_rate\": samplerate}\n",
    "\n",
    "        print(\"Step 2: Identifying speakers (Diarization)...\")\n",
    "        diar_output = self.diarization_pipeline(audio_payload)\n",
    "        \n",
    "        diar_segments = []\n",
    "        for turn, speaker in diar_output.exclusive_speaker_diarization:\n",
    "            diar_segments.append({\"start\": turn.start, \"end\": turn.end, \"speaker\": speaker})\n",
    "\n",
    "        print(\"Step 3: Transcribing with Groq Whisper-v3[cite: 27]...\")\n",
    "        with open(audio_path, \"rb\") as file:\n",
    "            transcription = self.groq_client.audio.transcriptions.create(\n",
    "                file=(audio_path, file.read()),\n",
    "                model=\"whisper-large-v3\",\n",
    "                response_format=\"verbose_json\"\n",
    "            )\n",
    "\n",
    "        print(\"Step 4: Speaker-Aware Indexing to Supabase[cite: 56]...\")\n",
    "        for w_seg in transcription.segments:\n",
    "            midpoint = (w_seg['start'] + w_seg['end']) / 2\n",
    "            current_speaker = \"UNKNOWN\"\n",
    "            for d_seg in diar_segments:\n",
    "                if d_seg['start'] <= midpoint <= d_seg['end']:\n",
    "                    current_speaker = d_seg['speaker']\n",
    "                    break\n",
    "            \n",
    "            role = role_mapping.get(current_speaker, \"OTHER\")\n",
    "            text = w_seg['text'].strip()\n",
    "            embedding = self.embed_model.encode(text).tolist()\n",
    "            \n",
    "            # Indexing requirements: include speaker metadata \n",
    "            self.supabase.table(\"clinical_segments\").insert({\n",
    "                \"content\": text,\n",
    "                \"speaker_role\": role,\n",
    "                \"embedding\": embedding,\n",
    "                \"metadata\": {\"start\": w_seg['start'], \"end\": w_seg['end']}\n",
    "            }).execute()\n",
    "\n",
    "    def get_full_transcript(self):\n",
    "        \"\"\"Retrieves every segment in chronological order for the full transcript\"\"\"\n",
    "        print(\"\\n--- FETCHING FULL SPEAKER-SEPARATED TRANSCRIPT ---\")\n",
    "        # Fetching all segments ordered by metadata->start time\n",
    "        response = self.supabase.table(\"clinical_segments\") \\\n",
    "            .select(\"speaker_role, content, metadata\") \\\n",
    "            .order(\"metadata->start\", desc=False) \\\n",
    "            .execute()\n",
    "        \n",
    "        transcript_text = \"\"\n",
    "        for record in response.data:\n",
    "            line = f\"[{record['speaker_role']}]: {record['content']}\"\n",
    "            print(line)\n",
    "            transcript_text += line + \"\\n\"\n",
    "        return transcript_text\n",
    "\n",
    "    def generate_clinical_summary(self, transcript):\n",
    "        \"\"\"Summarization Engine grounded in retrieved segments \"\"\"\n",
    "        print(\"\\n--- GENERATING LLM SUMMARY ---\")\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "        Summarize the following clinical interview. \n",
    "        Focus on Patient concerns and Clinician observations.\n",
    "        \n",
    "        TRANSCRIPT:\n",
    "        {transcript}\n",
    "        \n",
    "        SUMMARY FORMAT:\n",
    "        1. Patient Reported Symptoms:\n",
    "        2. Clinician Observations/Questions:\n",
    "        3. Follow-up Plan:\n",
    "        \"\"\"\n",
    "\n",
    "        completion = self.groq_client.chat.completions.create(\n",
    "            model=\"llama-3.3-70b-versatile\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        return completion.choices[0].message.content\n",
    "\n",
    "# --- EXECUTION ---\n",
    "if __name__ == \"__main__\":\n",
    "    AUDIO_FILE = \"audio.wav\"\n",
    "    ROLES = {\"SPEAKER_00\": \"CLINICIAN\", \"SPEAKER_01\": \"PATIENT\"}\n",
    "\n",
    "    bot = ClinicalIRSystem()\n",
    "    \n",
    "    # 1. Ingest Audio\n",
    "    # bot.process_audio_file(AUDIO_FILE, ROLES)\n",
    "    \n",
    "    # 2. Get Full Transcript\n",
    "    full_transcript = bot.get_full_transcript()\n",
    "    \n",
    "    # 3. Generate Summary \n",
    "    summary = bot.generate_clinical_summary(full_transcript)\n",
    "    print(\"\\n--- CLINICAL SUMMARY ---\\n\", summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
